<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="A new approach for self-supervised audio-visual separation using hierarchical fusion and representation alignment.">
  <meta property="og:title" content="Audio-Visual Separation with Hierarchical Fusion and Representation Alignment"/>
  <meta property="og:description" content="This work proposes a novel method for audio-visual source separation by integrating a hierarchical fusion strategy and a representation alignment approach."/>
  <meta property="og:url" content="https://happy-new-bears.github.io/AVSep-HFRA"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Audio-Visual Separation with Hierarchical Fusion and Representation Alignment">
  <meta name="twitter:description" content="A new approach for self-supervised audio-visual separation using hierarchical fusion and representation alignment.">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="audio-visual separation, deep learning, hierarchical fusion, representation alignment, self-supervised learning, BMVC">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Audio-Visual Separation with Hierarchical Fusion and Representation Alignment</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Audio-Visual Separation with Hierarchical Fusion and Representation Alignment</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="#" target="_blank">Han Hu</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Dongheng Lin</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Qiming Huang</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Yuqi Hou</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Hyung Jin Chang</a>,</span>
                <span class="author-block">
                  <a href="#" target="_blank">Jianbo Jiao</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">The MIx Group, School of Computer Science, University of Birmingham<br>BMVC 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/BMVC_Han_final.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/happy-new-bears/AVSep-HFRA" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-supervised audio-visual source separation leverages natural correlations between audio and vision modalities to separate mixed audio signals. In this work, we first systematically analyze the performance of existing multimodal fusion methods for audio-visual separation task, demonstrating that the performance of different fusion strategies is closely linked to the characteristics of the sound-middle fusion is better suited for handling short, transient sounds, while late fusion is more effective for capturing sustained and harmonically rich sounds. We thus propose a hierarchical fusion strategy that effectively integrates both fusion stages. In addition, training can be made easier by incorporating high-quality external audio representations, rather than relying solely on the audio branch to learn them independently. To explore this, we propose a representation alignment approach that aligns the latent features of the audio encoder with embeddings extracted from pre-trained audio models. Extensive experiments on MUSIC, MUSIC-21 and VGGSound datasets demonstrate that our approach achieves state-of-the-art results, surpassing existing methods under the self-supervised setting. We further analyze the impact of representation alignment on audio features, showing that it reduces modality gap between the audio and visual modalities. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Relationship Between Acoustic Properties and Fusion</h2>
    <div class="columns is-centered">
      <div class="column">
        <img src="static/images/figure1.png" alt="Relationship between Acoustic Properties of Musical Instruments and Fusion Strategies" style="width:100%;">
        <h2 class="subtitle has-text-centered">
          Relationship Between Acoustic Properties of Musical Instruments and Fusion Strategies. Instruments with shorter transient properties and simpler harmonic structures are more suited to middle fusion. Conversely, instruments with sustained notes and complex harmonic structures benefit more from late fusion.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Model Overview</h2>
    <div class="columns is-centered">
      <div class="column">
        <img src="static/images/figure2.png" alt="Pipeline of our proposed method" style="width:100%;">
        <h2 class="subtitle has-text-centered">
          Pipeline of our proposed method. The pipeline consists of three key components: audio-visual feature extraction, hierarchical fusion, and representation alignment. It takes an audio mixture and corresponding video frames as input.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Results</h2>

    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Qualitative Results</h3>
        <img src="static/images/figure3.png" alt="Qualitative Performance on MUSIC dataset" style="width:100%;">
        <h2 class="subtitle">
          Qualitative Performance on MUSIC dataset. Our method (the fourth row) produces cleaner and more distinct separated sounds compared to CLIPSep (the third row).
        </h2>
      </div>
    </div>
    
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h3 class="title is-4">Quantitative Results</h3>
        
        <div class="columns is-centered has-text-centered">
          <div class="column">
            <img src="static/images/table1.png" alt="Table 1: Performance comparison on MUSIC dataset">
            <h2 class="subtitle">
              Table 1: Audio-visual separation performance comparison on the MUSIC dataset. 
            </h2>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column">
            <img src="static/images/table3.png" alt="Table 3: Performance comparison on VGGSound dataset">
            <h2 class="subtitle">
              Table 2: Audio-visual separation results on VGGSound. 
            </h2>
          </div>
        </div>
        
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{hu2025audio,
  title={Audio-Visual Separation with Hierarchical Fusion and Representation Alignment},
  author={Hu, Han and Lin, Dongheng and Huang, Qiming and Hou, Yuqi and Chang, Hyung Jin and Jiao, Jianbo},
  booktitle={British Machine Vision Conference (BMVC)},
